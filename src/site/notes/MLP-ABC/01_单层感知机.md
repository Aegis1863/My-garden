---
{"dg-publish":true,"permalink":"/mlp-abc/01/","dgPassFrontmatter":true}
---


# 单个神经元的结构

感知机也就是神经元，是 MLP (多层感知机： Multi-Layer Perceptron)的基本单元，这里先学习单个感知机是如何传递信号的。我们在神经网络图像中一般用一个圆来表示一个感知机，一个感知机自带两个公式，一个是 $y=wx+b$，$w$ 也叫权重，$b$ 也叫偏置项，我相信这不难理解，另一个是激活函数，与生物神经元性质最接近的激活函数是 $sigmoid$，如图 1.1 和公式 1。而单个感知机的结构如图 1.1（b），我相信它的信息是不言而喻的：x 进入感知机后经过两步运算得到一个输出，这个输出传递给下一层所有的神经元，而每个神经元几乎都是一样的，这没有什么新鲜的，对吧！

注意：此时 $y$ 和 $x$ 是已知的，比如事先知道了 100 个 x 与 y 的值，求的是 $w$ 和 $b$，求出来以后，给定任意的 $x$ 就可以得到一个 $y$，统计学中的最小二乘法就是要解决这个问题。$w$ 和 $b$ 在一开始会被随机赋值，比如从正态分布中抽两个随机数给它们，在后续计算中，它们的值会一直被修改，在修改若干次后停止，最终的值就是我们的估计，换句话说，它们是*需要被估计的参数*，而 $sigmoid$ 函数里面没有未知数，激活函数只是改变这个神经元最终输出的形状，没有未知参数。

<figure id="figure3">
<img src="https://s2.loli.net/2023/08/27/3UAEKC6oGN1jDZO.jpg"/>
</figure>

$$
\begin{equation}
    \begin{cases}
        y=wx+b \\
        sigmoid (y) = \frac{1}{1+e^{-y}} \tag{公式1}
    \end{cases}
\end{equation}
$$

公式 1 是神经元中包含的两个公式，可以合并起来，把 $y=wx+b$ 带入

$$\tag{公式2}
sigmoid：
\begin{equation}
    Y = \frac{1}{1+e^{-wx+b}}
\end{equation}$$

如果此时我要求你对公式 2 计算 $\frac{dy}{dx}$，那么你（大概）很快就能写出公式。实际上 $sigmoid$ 函数很适合做求导，我们把它简单写成 $\sigma(x)$:

$$
\begin{equation} \tag{公式3}
    \sigma' (x) = \frac{e^{-x}}{(1+e^{-x})^2}=\sigma (x)(1 - \sigma (x))
\end{equation}
$$
如果考虑公式 3，就仅仅需要把 $w$ 乘到里面，$w$ 是矩阵，求导之后需要转置，也就是 $\sigma(x)(1 - \sigma(x))w^T$，是不是真的很简单，对一个函数求导只需要调用它自己的函数两次，这非常适合用来编程，因为函数是可以重复利用的，不需要太多其他的运算。

总结一下，*神经元接收一个输入（也可以是多个），经过两步运算：1. 线性运算，2. 激活函数，得到一个输出，并且传给下一层所有的神经元，每个神经元都做同样的运算。*

# 激活函数

​​ $sigmoid$ 是一个 S 形函数，并且在 x 取无穷大或者无穷小时，其导数接近0，因此也叫饱和激活函数，类似的还有​，激活函数有很多种，**神经网络拟合函数是靠多个线条组合成的**，一个线条来源于一个感知机，这个线条可以是直线，也可以是曲线，它的形状取决于激活函数，激活函数是​时，单个线条就是一个曲线，当然，如果把曲线分割成无数段，它也可以是直线的形状。

正如上一节刚开始说的，人们早期使用​只是因为它类似真实生物的信号传递特征，但它并不是最优的，它的求导看起来确实非常简单，但能不能更简单呢，最简单的求导是一元一次方程，​的导数就是它的系数，并且，激活函数换成直线好像也并不冲突，被无限分割的直线还是直线，被无限分割的曲线呢？每一个部分也是"直线"。

于是现代最常用的激活函数​呼之欲出：
​​​​​​​ $$
    ReLU(x) =
    \begin{cases}
        0 & \text{if } x \le 0, \\
        x & \text{if } x > 0.
    \end{cases}
$$

当​的结果小于0的时候，经过 Relu 激活输出为0，也就是说它此时"死亡"，但不代表与它连接的下一个神经元也"死亡"，因为一个神经元实际上要接受上一层所有神经元的信号并且加起来，参考上图。每个神经元初始的​和​是不一样的，几乎不可能前面那层全都是 0。这有助于防止过拟合，试想，本来5个线段就可以拟合好的函数，现在放置了20个神经元，就有20条线段，那么至多有15个神经元是不必要的，它们凭空增加了计算复杂度，或者造成过拟合，但是残差连接可以改善此问题，在 [[MLP-ABC/04_模型改进技术\|04_模型改进技术]] 中具体讨论。ReLU 可以关闭接近一半的神经元：考虑到​和​刚开始是被认为初始化的，它们被赋予某种分布的某个随机值，通常是标准正态分布的，因此经过公式 1 第一个公式输出的​也是正态分布的，​小于0的那些神经元就被关闭。

<figure id="figure3">
<img src="https://s2.loli.net/2023/08/27/EpkSqiLnKWbwQoB.jpg"/>
</figure>

在输出层，如果是回归任务，则没有激活函数，如果是二分类任务，采用​激活函数，如果是多分类任务，采用​函数。