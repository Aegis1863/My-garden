---
{"dg-publish":true,"permalink":"/reinforcement-learning//","dgPassFrontmatter":true}
---


# PG和DQN{ #f11378}


[[Reinforcement Learning/DQN#^0cba58\|深度Q网络]]输出的是价值，所以有时候也说成价值网络V，它的目的是评价当前状态，采取各个离散动作的价值；或者，如果是连续动作下，它评价的是当前状态和采取某个动作的价值。
#机器学习/强化学习/Q网络 

[[Reinforcement Learning/PG\|策略网络]]输出的是动作，可以直接输出动作，连续动作可以用tanh映射输出并且缩放到动作空间上，也可以输出均值方差来构造一个连续或离散的分布，然后从里面抽动作；离散动作还可以直接通过softmax输出动作概率，再抽取动作。
#机器学习/强化学习/策略网络

# 目标函数和损失函数

#机器学习/深度学习/目标函数

我们建立模型之后一般不会写出目标函数，但是目标函数是客观存在的，torch可以对目标函数进行自动微分，求出梯度之后就可以更新梯度。

#机器学习/深度学习/损失函数 

深度学习里面一般说损失函数，深度学习是有监督的，有labels，损失函数就衡量模型结果与真实labels的差异，这个差异越小越好

损失函数和目标函数其实是类似的概念，在强化学习中一般叫目标函数，因为对于损失函数来说通常做梯度下降，但是目标函数有时是做梯度上升的，比如强化学习中有时候要对奖励梯度上升以最大化奖励。