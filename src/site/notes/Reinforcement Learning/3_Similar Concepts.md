---
{"dg-publish":true,"permalink":"/reinforcement-learning/3-similar-concepts/","dgPassFrontmatter":true,"created":"2023-08-17T23:08:28.997+08:00","updated":"2023-10-22T19:04:53.731+08:00"}
---

# 1. 价值策略和梯度策略
## 价值策略

[[Reinforcement Learning/1_DQN#^0cba58\|深度Q网络]]输出的是价值，所以有时候也说成价值网络V，它的目的是评价当前状态，采取各个离散动作的价值；或者，如果是连续动作下，它评价的是当前状态和采取某个动作的价值。
#机器学习/强化学习/价值网络 

## 梯度策略

[[Reinforcement Learning/2_Policy Gradient\|策略网络]]输出的是动作，可以直接输出动作，连续动作可以用tanh映射输出并且缩放到动作空间上，也可以输出均值方差来构造一个连续或离散的分布，然后从里面抽动作；离散动作还可以直接通过softmax输出动作概率，再抽取动作。
#机器学习/强化学习/策略网络

# 2. 目标函数和损失函数

我们建立模型之后一般不会写出`目标函数`，但是目标函数是客观存在的，torch可以对目标函数进行自动微分，求出梯度之后就可以更新梯度。

深度学习里面一般说`损失函数`，深度学习是有监督的——有标签，损失函数就衡量模型结果与真实标签的差异，这个差异越小越好

损失函数和目标函数其实是类似的概念，在强化学习中一般叫目标函数，因为对于损失函数来说通常做梯度下降，但是目标函数有时是做梯度上升的，比如强化学习中有时候要对奖励梯度上升以最大化奖励。

# 同策略和异策略
#机器学习/强化学习/同策略 #机器学习/强化学习/异策略 
## 同策略

同策略指利用当前的策略与环境交互并且训练自己，训练一次，策略迭代一次，当前策略去环境中交互一次，再获得 next_state 和 reward，用这些数据再训练自己迭代一次，循环往复。所以异策略一直是用自己当前交互得到的经验训练自己。

## 异策略

异策略指用来训练的经验并不是自己当前状态交互获得的，可以是别的智能体或者人交互得到的。一般为了方便，我们直接利用一个未经训练的神经网络去交互，动作通常很随机，采样到大量经验。我们训练的时候，采样这些经验训练，每训练一次策略也就迭代了一次策略，每轮迭代的策略都是前一个策略的改进，所以策略一直在变化。最开始的经验是那个未经迭代的策略得到的，所以相当于智能体一直在用“别的”经验训练。这就是异策略。

异策略的效率更高，因为大量经验可以反复利用，而同策略的经验用一次就要丢掉了

## 两者结合

[[Reinforcement Learning/5_PPO\|PPO]] 尝试将两种策略结合，在大框架中是同策略，通过重要性采样的方法，对同一个经验多次异策略学习，但是通常还是归为同策略，具体资料参考 [[Reinforcement Learning/5_PPO\|PPO]]。
