---
{"dg-publish":true,"permalink":"/reinforcement-learning/3-similar-concepts/","dgPassFrontmatter":true,"created":"2023-08-17T23:08:28.997+08:00"}
---

# 1. 价值策略和梯度策略
## 1.1. 价值策略

[[Reinforcement Learning/1_DQN#^0cba58\|深度Q网络]]输出的是价值，所以有时候也说成价值网络V，它的目的是评价当前状态，采取各个离散动作的价值；或者，如果是连续动作下，它评价的是当前状态和采取某个动作的价值。
#机器学习/强化学习/价值网络 

## 1.2. 梯度策略

[[Reinforcement Learning/2_Policy Gradient\|策略网络]]输出的是动作，可以直接输出动作，连续动作可以用tanh映射输出并且缩放到动作空间上，也可以输出均值方差来构造一个连续或离散的分布，然后从里面抽动作；离散动作还可以直接通过softmax输出动作概率，再抽取动作。
#机器学习/强化学习/策略网络

# 2. 目标函数和损失函数

## 2.1. 目标函数

我们建立模型之后一般不会写出`目标函数`，但是目标函数是客观存在的，torch可以对目标函数进行自动微分，求出梯度之后就可以更新梯度。

## 2.2. 损失函数

深度学习里面一般说`损失函数`，深度学习是有监督的——有标签，损失函数就衡量模型结果与真实标签的差异，这个差异越小越好

损失函数和目标函数其实是类似的概念，在强化学习中一般叫目标函数，因为对于损失函数来说通常做梯度下降，损失当然越小越好，但是强化学习是希望奖励越大越好的，所以就叫目标函数了。

# 3. 同策略和异策略

## 3.1. 同策略
#机器学习/强化学习/同策略 

同策略指利用当前的策略与环境交互并且训练自己，训练一次，策略迭代一次，当前策略去环境中交互一次，再获得 next_state 和 reward，用这些数据再训练自己迭代一次，循环往复。所以异策略一直是用自己当前交互得到的经验训练自己。

## 3.2. 异策略
#机器学习/强化学习/异策略 

异策略指用来训练的经验并不是自己当前状态交互获得的，可以是别的智能体或者人交互得到的。一般为了方便，我们直接利用一个未经训练的神经网络去交互，动作通常很随机，采样到大量经验。我们训练的时候，采样这些经验训练，每训练一次策略也就迭代了一次策略，每轮迭代的策略都是前一个策略的改进，所以策略一直在变化。最开始的经验是那个未经迭代的策略得到的，所以相当于智能体一直在用“别的”经验训练。这就是异策略。

异策略的效率更高，因为大量经验可以反复利用，而同策略的经验用一次就要丢掉了

## 3.3. 两者结合

[[Reinforcement Learning/5_PPO\|PPO]] 尝试将两种策略结合，在大框架中是同策略，通过重要性采样的方法，对同一个经验多次异策略学习，但是通常还是归为同策略，具体资料参考 [[Reinforcement Learning/5_PPO\|PPO]]。

# 4. 在线学习和离线学习

这个概念容易和前面的同策略、异策略搞混，实际上不是一回事。

## 4.1. 在线学习

在线学习是指事先不准备任何数据，通过与环境的交互来学习。

## 4.2. 离线学习

离线学习是指事先准备了数据，可能来源于其他智能体，或者是人类实践得到的，存在服务器或者本地，这样我们需要训练的智能体完全不需要同环境交互，就可以有数据去训练；和异策略是有点类似的，但是异策略的经验也是交互来的。这就要求预先处理好数据，并且智能体所需要适应的环境要比较稳定才行，否则预先准备的数据就不能用了。

现代强化学习的基础算法大部分都是在线学习的。

# 5. 重参数化采样

critic 更新是通过时序差分方法更新的，有的 critic 需要同时接收状态和动作作为输入，有的则只需要状态作为输入。一旦动作也作为输入，就必须考虑动作能不能传导梯度，如果动作不能传递梯度，就无法更新 critic。

如果 actor 输出的是正态分布的参数 mu 和 std，那么需要建立一个正态分布来采样动作，但是采样的动作是无法传递梯度的：试想，一个被抽样的样本如何对其所属分布的参数求导呢？这是不可能的，所以必须要重参数化。可以通过重参数化技巧来传递梯度，也有 gamble-softmax 等方法。

如果 actor 输出的直接是动作，比如 DDPG，那么动作本身就可以直接传递梯度，无需重参数化采样。是否需要动作传导梯度，也取决于 critic 的输入是否包括动作。

## 5.1. 不同算法辨析

[[Reinforcement Learning/5_PPO\|PPO]] 的actor输出概率直接采动作，没有重参数化，但是 critic 也不接收动作，所以梯度传导的时候与动作无关。

[[Reinforcement Learning/6_DDPG\|DDPG]] 的 actor 是直接输出动作，没有重参数化采样，critic 接收动作，梯度传导时与动作有关，需要动作传递梯度，但是因为是直接输出动作，本身就可以传递梯度。

[[Reinforcement Learning/8_Soft Actor Critic (SAC)\|SAC]] 的 actor 输出动作，其中重参数化采样获得动作，这样可以传导梯度，critic 接收动作，所以传导梯度是需要动作的。