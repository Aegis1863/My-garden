---
{"dg-publish":true,"permalink":"/reinforcement-learning/12-muti-agent-rl/","dgPassFrontmatter":true,"created":"2023-10-20T15:22:25.590+08:00","updated":"2023-10-21T17:57:07.398+08:00"}
---

[19\_多智能体.ipynb](https://github.com/Aegis1863/ML_practice/blob/master/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/19_%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93.ipynb)
# 1. 多智能体强化学习

## 1.1. 多智能体和类似任务的状态设计

可以发现很多类似任务设计中，如果是比较复杂的状态，比如是数组的组合，复合数组等，都是“暴力”拼接、拉直的，类似 CNN 中把最后提取的特征展平到 MLP 一样，对人类来说这样做很难学到任何东西，但是对神经网络来说是有效的。

## 1.2. 多智能体编程技巧

其实很简单，比如算法是 PPO，只需要建立一个列表，装入若干 PPO 算法即可，可以采取中心化训练，去中心化执行的方法：评论员网络接收全部智能体状态（或和动作，全部拼接展平），每个智能体，仅采用它观察到的状态采取动作。

因此，进一步应该想到，每个智能体的状态应该单独给出，整理在一个表里输出作为总状态。并且在训练智能体的时候，善用 `enumerate()` 和 `zip()`，同时输出序号和内容，便于和不同智能体数据对应。