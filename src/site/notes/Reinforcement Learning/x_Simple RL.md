---
{"dg-publish":true,"permalink":"/reinforcement-learning/x-simple-rl/","dgPassFrontmatter":true,"created":"2024-05-21T14:47:54.031+08:00"}
---

# 1. 强化学习直观理解

强化学习的应用场景是马尔可夫过程，很多现实中的问题都可以认为是马尔可夫过程，特征是当前状态仅仅与前一个状态有关，而与更早的状态无关。

按照随机过程的定义：

$$
\begin{aligned}
&P\{X(t_n)<x_n|X(t_{n-1})=x_{n-1}, ...,  X(t_2)=x_2,  X(t_1)=x_1\} \\
=&P\{X(t_n)<x_n|X(t_{n-1})=x_{n-1}\}
\end{aligned}
$$

比如下象棋、围棋，玩家在 t 时刻下了一步棋，下一步 t+1 时刻棋该怎么下，只取决于上一步下完以后的棋局，而与更早是怎么下的无关，类似的例子还有走迷宫、机器人运动时如何控制关节等问题。可以理解为强化学习要求模型在马尔可夫过程的每个节点做一个决策，从而组成一系列决策，希望找到最佳的决策方案，比如训练一个模型能够下一手好棋（AlphaGo）。目前强化学习应用的最好的领域之一是训练游戏 AI：[只狼](https://www.bilibili.com/video/BV1Dg4y137Cq)， [马里奥](https://www.bilibili.com/video/BV1CM4y1N7Wi) ，[雅达利游戏](https://www.bilibili.com/video/BV1e5411R7rF)。

在有监督学习中，模型（神经网络）的学习是通过梯度下降去调整参数的，而梯度是根据预测结果与真实值或标签计算损失来的，强化学习就是需要在没有真实值或标签的情况下，也学习好一个模型（神经网络）。但是如果没有任何指示，强化学习也不可能有任何效果，相当于空手套白狼，为了解决这个问题，就引入了奖励机制。设计一个基本的奖励是很简单的，我们要完成任何任务，效果如何都有评价方法，比如中学生学习的好不好可以由周考月考来检验，但是这种奖励不一定是及时的，它可能是延迟体现的，比如我学习了一天，奖励是0，学习了一个星期还是0，直到一个月后考试得了高分，奖励是100，但是这种情况不影响强化学习。有了奖励机制，可以通过强化学习的手段去从奖励中学习并且调整参数，所谓强化学习的手段通常是指 **Q 学习**或者**策略梯度**（Sutton,  et al.）。

 **Q 学习**或者**策略梯度**在这里不详细说，只需要知道它们可以给出一个损失函数去训练模型。损失函数的目标是根据奖励和价值估计去构造的，但是这个目标不是一个 ground truth，所以不能说强化学习是有监督学习；同时，又因为用到了梯度下降去调参，所以也不能说是无监督的，强化学习是平行于有监督和无监督的一个大类。

<img src="C:\Users\Bowen\AppData\Roaming\Typora\typora-user-images\image-20240526122632351.png" alt="image-20240526122632351" style="zoom:50%;" />

为了直观理解强化学习，这里说一个例子，看看强化学习怎么解决大家比较熟悉的 CNN 图片分类任务：

有监督学习中 CNN 网络做图片分类，首先把图片输入 CNN，模型输出几个类别概率，选择概率最大的类别，然后与真实标签对比，计算交叉熵损失并且梯度下降。

强化学习也可以解决这个问题，首先假设模型无法获得真实标签（我们有，但是不能告诉模型），同样构建一个 CNN 网络，将图片输入 CNN，网络给出一个类别的判断，到这里正如有监督学习中一样，唯一不同的是此时没有标签，也就不能通过交叉熵去计算损失。为了解决这个问题，我们可以简单地设定一个奖励机制，即如果网络预测对了（未经训练的网络其实就是乱猜），我们就给一个正的奖励分，比如 1，如果预测错了，就给一个负的奖励分，比如-1. 我们可以让 CNN 网络去猜测很多次从而构成一个决策序列（网络预测的结果看作是在做决策），有了这个决策序列，再通过**Q 学习**或者**策略梯度**方法就可以让 CNN 去调整参数使得准确性越来越高，因为Q 学习或者策略梯度就是在引导 CNN 去学习一种决策方法，该决策方法会使得模型在这一系列决策中得到最高的奖励。

在这里最高的奖励就是每次都猜对了，猜测 n 轮就有 n 分，猜错了一次就是 n-1 分，所以强化学习可以让 CNN 争取每次都预测对，达到和有监督学习一样的效果。实验证明强化学习也确实可以训练好 CNN 网络，但是需要更长的时间，效率远不如有监督学习。

通过以上描述，应该可以感受到强化学习是一种非常复杂的方法，并且效率远不如有监督学习，效果也不一定会好。强化学习的优势仅仅在于这类任务：确实不知道真实标签，而恰好有一个方法去给决策以奖励（这个奖励与梯度无关，否则就可以做有监督了）。

## 1.1. 奖励机制

还是以下棋来举例，每一步棋该怎么下是没有最优解的（或者说很难求解），也就是说没有标签，有监督学习就无法应用，但是每一步棋其实是可以给出一个奖励分的，比如围棋中某一步吃掉对方一颗子，可以加 1分，如果吃掉很多子，可以加 5 分，如果吃掉很多子并且围了很多空（围棋术语），可以加 10 分。这就是一种奖励机制，是需要人为设计的。

说一个交通信号控制的例子来说明奖励机制：首先定义一个**状态**，这是一个向量，其中的分量就表示每个车道上的车辆数；**动作决策**：而神经网络去控制交通信号，其实就是一个分类任务，即对于当前状态，信号灯将其分类为哪种情况，信号灯有四种显示方案，就意味着四分类问题。**奖励**就设计为负的所有车道上的车辆数的总和，如果交通信号的控制使得总车辆数减少，那么就会有一个比较高的奖励（最高是0）。

可以注意到，强化学习中的三个关键要素是状态，动作和奖励，这是需要人为定义的，为了方便对算法本事的研究，这三种方案也有很多现成定义，通常被集成到强化学习环境中，通过简单的接口交互就可以接入算法。

前面说的 CNN 的例子中，模型预测对了，我们人为给一个正 1 的奖励分也是一种奖励机制。设计优秀的奖励机制是很有用的，但是如果研究目的是提升算法稳定性，就最好采用一个现成的环境，该环境是普遍认可的，已经预设了状态和奖励机制等。

# 2. 单个交通信号灯的强化学习环境



<img src="C:\Users\Bowen\Desktop\资源中心\Bowen's PKM\attachments\Pasted image 20240521145322.png" width="700"/>

该问题中，Environment 可以是一个 Python 定义的类，输入 action 并输出 state 和 reward。其中，action 表示动作选择；state 表示各个车道的信息，比如该车道的车辆密度等，是一个向量；reward 有多种设计方法，一种可行的设计是其值等于负的各个车道上车辆数的总和。在右侧的强化学习智能体框架内，state 表征（State representation）为表征状态，技术上体现为通过一个编码器，表征状态与 reward 一起输入**贝尔曼方程**（Bellman equation），通过**时序差分法**（Temporal difference update) 得到更新目标去训练机器学习模型。贝尔曼方程是动态规划中的一个重要工具，也是 Q 学习的内容，在后面会介绍，也可以用策略梯度方法。

循环顺序：
1. Env 给出一个初始状态 state
2. Agent 是一个神经网络，接受 state 作为输入，输出可以是状态动作价值 $V(a_t|s_t)$（离散动作空间），也可以直接输出 action（连续动作空间）
3. Env 根据 action 给出 next_state 和 reward。
4. Agent 通过 state，next_state，action 和 reward 更新参数。
5. 令 state=next_state，回到步骤 2

```Python
state, done = env.reset(), False
while not done:
	action = agent.take_action(state)
	next_state, reward, done = env.step(action)
	agent.update(state, next_state, reward)
	state = next_state
```

# 3. 基本推导

这一节简单写一下推导方法，其实真正应用的时候，推导不是很重要，直接拿推导出来的目标函数去用就行了。所以没耐心看也无所谓，~~公式都是骗哥们的~~，直接跳转到第四节。
## 3.1. Q 学习

首先给出回报（Gain，G）的概念，回报等于当前决策以及后续所有决策的经过折算的奖励分的总和，因为要了解当前决策的价值，不仅取决于及时奖励分，还取决于后续的奖励分，就像一步好棋可能会提升后续所有棋的奖励，因为这一步棋很关键，坏棋同理。所以到底怎么算呢，对于时间步 t 的回报 $G_t$ 有：

$$
\begin{aligned}
G_t&=r_t+\gamma r_{t+1}+\gamma^2t_{t+2}+...+\gamma^nr_{t+n}\\
&=\sum^n_{i=0}\gamma^ir_{t+i}
\end{aligned}
$$

公式里面有个 $\gamma$ ，取值在 0~1 之间，这是用来折算未来奖励的，因为未来的奖励对当前回报的影响是视任务需要而定的。比如给你一百块钱，你可以立即拿，也可以一年后拿，考虑通货膨胀因素的话当然是现在拿了好，因为钱会因为通货膨胀贬值，所以立即拿是最好的，所以重视当前回报。也有反例，比如你开救护车，很有可能因为病人病危而闯红灯，闯红灯很危险而且会被罚款扣分，所以违反交通规则会得到负的奖励，然而你一旦及时送到病人，就可能有大得多的奖励，所以这种情况下远期奖励很重要，当然，这是一种假设，不能在现实中实验。一般来说大部分任务的 $\gamma$ 取 0.9.

前面已经说了，在每一个时间步上是找不到最优决策的，就是说没有标签，只知道奖励信号，所以贝尔曼方程给出了一个价值估计方法，让模型估计每一步的价值，设 $V(s_t)$ 是在 t 时刻的状态 $s_t$ 的价值，其计算方法是求回报 G 的期望：
$$
\begin{equation}\begin{aligned}
V_\pi (s_t)& =\mathbb{E}_\pi[G_t|S_t=s] \\
&=\mathbb{E}_\pi\left[\sum_{k=0}^\infty\gamma^kr_{t+k}|S_t=s\right] \\
&=\mathbb{E}_\pi\left[\sum_{k=0}^\infty r_t+\gamma\sum_{k=0}^\infty\gamma^kr_{t+k+1}|S_t=s\right] \\
&=\mathbb{E}_\pi\left[\sum_{k=0}^\infty r_t+\gamma V_\pi (S_{t+1})|S_t=s\right]
\end{aligned}\end{equation}
$$
根据上面推导可以给一个 $G_t$ 与 $V_\pi$ 的关系：
$$
G_t=r_t+\gamma V_\pi(s_{t+1}) \tag{1}
$$
并且可以认为状态的价值等于该状态的回报的期望：
$$
V_{\pi} = \bar{G} \tag{2}
$$
一开始我们肯定不知道每一步的价值是多少，但是我们可以多次试错去估计价值，因为每做一个决策到达一个状态都会得到一个奖励分（reward），这就是**蒙特卡洛方法**，对于某个状态得到的奖励 $G$ 的价值估计就是多次在该状态得到的奖励的均值 $\bar{G}$，不失一般性地假设被访问了 $t$ 次，计算为：
$$
\begin{equation}\begin{aligned}
\bar{G}&=\frac1t\sum_{j=1}^tG_j \\
&=\frac{1}{t}\left(G_t+\sum_{j=1}^{t-1}G_j\right) \\
&=\frac{1}{t}\left(G_{t}+(t-1)\bar{G}_{t-1}\right) \\
&=\frac{1}{t}\left(G_{t}+t\bar{G}_{t-1}-\bar{G}_{t-1}\right) \\
&=\bar{G}_{t-1}+\frac{1}{t}\left(G_{t}-\bar{G}_{t-1}\right)
\end{aligned}\end{equation}
$$


以上方法也称为期望的增量计算。根据公式 1 和公式 2，把下标变一下，$\frac{1}{t}$ 可以看做常数，改写成 $\alpha$，从而把上式改写为：
$$
\begin{aligned}
&V(s_t) \leftarrow V(s_t) - \alpha(r_t+V(s_{t+1})-V(s_t))\\
\Rightarrow ~~ & loss = MSE(r_t+V(s_{t+1}),  V(s_t))
\end{aligned}
$$
这样 Q 学习的损失函数就写出来了，称为**时序差分法**，直接用 MSE 计算损失并且梯度下降即可。这样神经网络模型 $\pi_\theta(a_t|s_t)$ 就可以对动作状态价值做出一个尽可能准确的估计，一旦有了这个估计，每当碰到一个状态 $s$ 的时候，就可以估计出在这个状态下做出每个动作的价值是多少，直接选择那个价值最高的动作就可以了。
## 3.2. 策略梯度

策略梯度是一种不同于 Q 学习的强化学习方法，它不是用时序差分法来做价值估计，而是通过提升模型做出决策获得奖励的概率来做策略优化。例如用一个神经网络 $\pi_\theta(a_t|s_t)$ 对于状态 $s_t$ 给出的各个动作 $a_t$ 的概率 $p_\theta$ ，乘以当前得到的奖励 $r$，我们可以对这个结果进行==梯度上升==。这里提升的是当前动作的概率乘以执行这个动作得到的奖励，其中概率是神经网络给出来的，是带有梯度的，所以可以做梯度上升。学过有监督都知道梯度下降，在损失函数前面加个负号就是梯度上升。总之，这个目标函数可以构造为：
$$
\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)
$$
其中 $\tau$ 代指一整条轨迹，$p_\theta(\tau)$ 是该轨迹出现的概率，$\theta$ 是神经网络参数，之前说了动作概率是神经网络给出来的，$R(\tau)$ 指奖励序列，可以认为是 $[r_1, r_2,  ...]$，这里是对多个轨迹的奖励求期望。

补充说明一下 $p_\theta$ 具体是什么，对于轨迹 $\tau$ 出现的概率 $p_\theta(\tau)$，可以通过以下公式计算：
$$
p_\theta(\tau) = p(s_1)p_\theta(a_1|s_1)p(s_2|s_1, a_1)p_\theta(a_2|s_2)p(s_3|s_2, a_2)... \tag{3}
$$
为什么有的 $p$ 带了下标 $\theta$ ，有的没有带，因为 $\theta$ 是神经网络参数，像 $p (s_1)$ 这种不带 $\theta$ 的是环境给出来的，不是神经网络决定的，环境是一种客观事实，但是在 $s_1$ 状态下给出动作 $a_1$ 的概率是神经网络实现的，是模型的主观决定，其他的项同理。

回顾本节最开始说的，策略梯度的目的是提升奖励，所以我们尝试求出期望奖励的梯度：
$$
\begin{equation}\begin{aligned}
\nabla \bar{R}_{\theta}&=\sum_{\tau} R(\tau) \nabla p_{\theta}(\tau)\\
&=\sum_{\tau} R(\tau) p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\&= \sum_{\tau} R(\tau) p_{\theta}(\tau) \nabla \log p_{\theta}(\tau) \\
&=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right]\\&\approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(\tau^{n}\right) \\ 
&=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
\end{aligned}\end{equation}
$$


式子中最后一步是根据前面公式 3 得到的，因为是对 $\theta$ 求导，所以把 $p_\theta(\tau)$ 中与 $\theta$ 无关的项全部消掉就推出来了。$R(\tau^n)$ 是可以获得的，$p_{\theta}(a_{t}^{n}|s_{t}^{n})$ 是神经网络给出的，都可以计算，算出来之后自动微分就行了。

我们可以直观地理解梯度策略最后那个式子，也就是在模型采样到的数据里面，采样到在某一个状态 $s_t$ 要执行某一个动作 $a_t$，$(s_t​,a_t​)$ 是在整个轨迹 $\tau$ 的里面的某一个状态和动作的对。假设模型在 $s_t$ ​ 时执行 $a_t$ ​，最后发现 $\tau$ 的奖励是正的，模型就要增加在 $s_t$ 执行 $a_t$ ​ 的概率。反之，如果在 $s_t$ 执行 $a_t$ ​ 会导致 $\tau$ 的奖励变成负的，模型就要减少在 $s_t$ ​ 执行 $a_t$ ​​ 的概率。

# 4. 应用案例

## 4.1. 商用成功

- 机器人（机械臂关节控制，比如宇树机器狗）：机器狗应用效果极好，人形机器人效果不佳。[宇树机器狗](https://www.bilibili.com/video/BV1Hw411B7qu/)
- 大语言模型（ChatGPT） 的人类反馈强化学习微调（RLHF）。
- 滴滴出行的司机派单决策实现总体收入优化的强化学习。

## 4.2. 表现不佳

- 强化学习一直被认为可以在自动驾驶方面发挥作用，但是其在自动驾驶方面的安全性始终表现不如 MPC（模型预测控制） 和有监督方案（特斯拉等车厂方案），中科院自动化所已经放弃这一路线，目前仅清华李升波组在尝试。
- 虽然大部分现实控制问题可以建模成马尔可夫决策过程从而应用强化学习，但是由于强化学习结构复杂（代码工程量大），学习门槛高（需要熟练掌握各种有监督深度学习方法），效率比较低（需要大量重复探索），效果难以保证，因此强化学习研究相对有监督来说要少很多。

# 5. 强化学习自身研究领域

1. 稳定性：强化学习的训练大都不太稳定，有时候存在剧烈波动。
2. 安全性：有时候算法为了获得高奖励而采用作弊或者危险的决策。
3. 效率：强化学习需要通过大量试错学习去迭代优化决策，提高学习速度和效率是一个重要研究。

# 6. 其他理解和研究

- **启发式算法**：强化学习可以理解为类似粒子群和模拟退火的启发式算法，并且是目前最先进的启发式算法之一，因此可以用来对 NP 难问题给出局部优化解，在这类问题一般用最简单的强化学习方法如 Q 学习就可以，由于这种任务需要定制化并且内容不复杂，所以需要自己定义一个奖励机制，且不太关注算法稳定性和效率，**这类研究中强化学习只是作为求一个可行解的手段**。

- **最优控制**：传统控制中的大部分应用都可以尝试采用强化学习进行研究，因此，可以从最优控制角度研究并且引入控制论的思想，比如 MPC，常见于机械臂的控制研究中。还有安全强化学习，通过组合优化和约束方法提高强化学习控制的安全性和效率。**这类研究侧重算法，旨在提升强化控制效率和精度**。

- **博弈论和智能决策**：强化学习的理念与迭代博弈问题不谋而合，强化学习所说的奖励设计与博弈论机制设计，以及博弈论所说的策略（strategy）在强化学习中称为政策（policy），其实含义基本一致，并且迭代博弈本身就是马尔科夫决策过程。所以在博弈论中越来越多引入强化学习作为模拟手段，同时，利用博弈论和因果推断方法可以反过来优化强化学习算法，**这类问题中强化学习是作为仿真模拟手段**。

# 7. 强化学习的前沿研究

- 模仿学习：变成有监督学习，即给出一个专家数据集，直接告诉模型哪个动作比较好，但是专家数据集往往是很难收集到的，并且即便有专家数据集，实际应用效果也不好，因为专家数据集里面收集的 state 很难说是普遍的（general），一旦模型遇到专家数据集里面没见过的情况表现就会极差，称为分布外问题（OOD，Out Of Distribution）。比较有名的模仿学习研究是英伟达的自动驾驶实验，具有一定的效果，但局限性很大。模仿学习已经被主流研究抛弃，转向离线强化学习。

- 离线强化学习：和模仿学习一样，给一个专家数据集，但是用有严谨数学推导的方法改进 Q 学习，使得模型效果能够达到甚至超越专家数据。困难仍然是专家数据集的可得性和质量，提出更有效的数学方法也是很困难的。

- 有模型的强化学习：灵感来源于控制论中的 MPC。用有监督方法训练一个 model，该 model 可以理解为虚拟环境，学习的是环境的状态转移和奖励机制的规律，为了区分model和强化模型，把强化模型称为智能体（agent）。Agent 可以在这个虚拟环境中做推演，从而在不实践的情况下实现更好的决策。比如 agent 本来想执行动作 1，它在 model 中执行动作 1，model 告诉它这个动作效果不会好，agent 就可以换一个动作，这种方法的重点在于planning。难点在于 model 是否可以训练的好，目前有用生成式模型 (diffusion) 去作为 model 的，属于最前沿方法。